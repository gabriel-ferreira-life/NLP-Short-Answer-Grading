# Automated Short Answer Grading (NLP 579 Final Project)

This repository presents a comparative study of three approaches to automatically grade short student responses using NLP models. We evaluate performance across a dataset of science-domain questions labeled as **Correct**, **Partially Correct**, or **Incorrect**.

## âš™ï¸ Repo Structure

```text
NLP-Short-Answer-Grading/
â”œâ”€â”€ config/
â”‚  â”œâ”€â”€ config.json           # Configuration file with data paths
â”‚  â””â”€â”€ requirements.txt      # Python dependencies
â”‚
â”œâ”€â”€ data/
â”‚  â”œâ”€â”€ train.csv             # Training dataset
â”‚  â””â”€â”€ test.csv              # Test dataset
â”‚
â”œâ”€â”€ problem_description/    # Project Description
â”‚
â”œâ”€â”€ exploration/            # Jupyter Notebooks
â”‚  â””â”€â”€ eda.ipynb             # Exploratory Data Analysis
â”‚
â”œâ”€â”€ solutions\
â”‚  â””â”€â”€ few_shot_exploration/
â”‚    â”œâ”€â”€ main.py               # Entry point: loads data, formats prompt, runs predictions
â”‚    â””â”€â”€ prompts/
â”‚      â”œâ”€â”€ few_shot.py          # Functions to format few-shot examples
â”‚      â””â”€â”€ template.py          # Prompt template used by the LLM
â”‚  â””â”€â”€ llama3.2/
â”‚    â”œâ”€â”€ nlp-short-answer-grading-llama-fine-tunning.ipynb
â”‚    â””â”€â”€ nlp-short-answer-grading-llama-inference.ipynb
â”‚  â””â”€â”€ bert/
â”‚    â”œâ”€â”€ BERT_Based_Model.ipynb
â”‚    â””â”€â”€ bert_based_model.py
â”‚  â””â”€â”€ rule_based/
â”‚    â””â”€â”€ Rule_Based_Model.ipynb
â”‚
â””â”€â”€ Report/                 # Project Report

```


## ğŸ§  Models Compared

### 1. Rule-Based Heuristic
- Uses keyword overlap between reference and student response.
- Simple and interpretable, but brittle to paraphrasing.

---

### 2. RoBERTa (BERT-based Classifier)
- Fine-tuned on concatenated input: `[CLS] Question [SEP] Reference Answer [SEP] Student Response`.
- Strong performance on "Correct" and "Incorrect", struggled with "Partially Correct".

---

### 3. LLaMA-3B (LLM-based Classifier)
- Instruction-tuned using LoRA adapters.
- Highest accuracy and the only model to capture partial correctness meaningfully.

## ğŸ“Š Results Summary

| Model        | Accuracy | Macro-F1 | Weighted-F1 |
|--------------|----------|----------|--------------|
| Rule-Based   | 70.0%    | 48.0%    | 71.0%        |
| RoBERTa      | 91.7%    | 61.0%    | 91.0%        |
| LLaMA-3B     | 94.8%    | 71.0%    | 95.0%        |

See heatmaps in the `images/` folder for a per-class breakdown.

---

## ğŸ“ Dataset

- 30,000+ labeled student responses
- Classes:
  - **2** = Correct
  - **1** = Partially Correct
  - **0** = Incorrect
- Notably imbalanced (only ~1% are Partially Correct)

Note: Data is hidden for privacy purposes.
---

## Report & Analysis
For a detailed report on findings and term analysis, refer to:
[Phrase Mining Report (PDF)](report/COMS_5790_Final_Project_Report.pdf). 

--

## ğŸ“„ Authors

Nicholas George \
Gabriel Ferreira \
Bhavika Gungurthi
